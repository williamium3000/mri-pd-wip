import argparse
import logging
import os
import pprint
import warnings
import itertools

import torch
from torch import nn
import torch.backends.cudnn as cudnn
from torch.optim import SGD, AdamW, Adam
from torch.optim import lr_scheduler
from torch.utils.data import DataLoader
import yaml
from dataset.pd_wip_2d import PDWIP2DDataset
from model.pix2pix import networks2d
from model.pix2pix.image_pool import ImagePool
from util.utils import count_params, init_log
from util.scheduler import *
from util.dist_helper import setup_distributed

from eval2d import evaluate_2d
from dataset.pd_wip_3d import PDWIP3DDataset

parser = argparse.ArgumentParser(description='Medical image segmentation in 3D')
parser.add_argument('--config', type=str, required=True)
parser.add_argument('--train-id-path', type=str, required=True)
parser.add_argument('--val-id-path', type=str, required=True)
parser.add_argument('--save-path', type=str, required=True)
parser.add_argument('--local-rank', default=0, type=int)
parser.add_argument('--port', default=None, type=int)
parser.add_argument('--clip-grad-norm', default=None, type=float)
parser.add_argument('--save_feq', type=int, default=None)
parser.add_argument('--save', action="store_true")

def backward_D_basic(netD, real, fake, criterionGAN):
    """Calculate GAN loss for the discriminator

    Parameters:
        netD (network)      -- the discriminator D
        real (tensor array) -- real images
        fake (tensor array) -- images generated by a generator

    Return the discriminator loss.
    We also call loss_D.backward() to calculate the gradients.
    """
    # Real
    pred_real = netD(real)
    loss_D_real = criterionGAN(pred_real, True)
    # Fake
    pred_fake = netD(fake.detach())
    loss_D_fake = criterionGAN(pred_fake, False)
    # Combined loss and calculate gradients
    loss_D = (loss_D_real + loss_D_fake) * 0.5
    loss_D.backward()
    return loss_D

def main():
    args = parser.parse_args()

    cfg = yaml.load(open(args.config, "r"), Loader=yaml.Loader)

    if args.save:
        args.out_dir = os.path.join(args.save_path, "results")
        os.makedirs(args.out_dir, exist_ok=True)
        
    logger = init_log('global', logging.INFO)
    logger.propagate = 0

    rank, word_size = setup_distributed(port=args.port)
    args.rank = rank
    if rank == 0:
        logger.info('{}\n'.format(pprint.pformat(cfg)))

    if rank == 0:
        os.makedirs(args.save_path, exist_ok=True)

    cudnn.enabled = True
    cudnn.benchmark = True

    
    model_G_AB = networks2d.define_G(**cfg["generator"])
    model_G_BA = networks2d.define_G(**cfg["generator"])
    model_D_AB = networks2d.define_D(**cfg["discriminator"])
    model_D_BA = networks2d.define_D(**cfg["discriminator"])
    
    if rank == 0:
        logger.info('Total params: {:.1f}M\n'.format(count_params(model_G_AB) + count_params(model_G_BA) + count_params(model_D_AB) + count_params(model_D_BA)))

    param_group_g = [{'params': model_G_AB.parameters(),'lr': cfg['lr']},
                    {'params': model_G_BA.parameters(), 'lr': cfg['lr']}]
    param_group_d = [{'params': model_D_AB.parameters(),'lr': cfg['lr']},
                    {'params': model_D_BA.parameters(), 'lr': cfg['lr']}]
    if cfg["optim"] == "SGD":
        optimizer_G = SGD(param_group_g, lr=cfg['lr'], momentum=0.9, weight_decay=1e-4)
        optimizer_D = SGD(param_group_d, lr=cfg['lr'], momentum=0.9, weight_decay=1e-4)
    elif cfg["optim"] == "AdamW":
        optimizer_G = AdamW(param_group_g, lr=cfg['lr'], weight_decay=0.01, betas=(0.9, 0.999))
        optimizer_D = AdamW(param_group_d, lr=cfg['lr'], weight_decay=0.01, betas=(0.9, 0.999))
    elif cfg["optim"] == "Adam":
        optimizer_G = Adam(param_group_g, lr=cfg['lr'], weight_decay=1e-4)
        optimizer_D = Adam(param_group_d, lr=cfg['lr'], weight_decay=1e-4)
    else:
        raise NotImplementedError(f'{cfg["optim"]} not implemented')
    trainset = PDWIP2DDataset(
        cfg=cfg,
        mode="train",
        pd_root=cfg['train_pd_root'],
        wip_root=cfg['train_wip_root'],
        list=args.train_id_path)
    valset = PDWIP3DDataset(
        cfg=cfg,
        mode="val",
        pd_root=cfg['val_pd_root'],
        wip_root=cfg['val_wip_root'],
        list=args.val_id_path, return_name=True)
    
    
    
    trainsampler = torch.utils.data.distributed.DistributedSampler(trainset)
    trainloader = DataLoader(trainset, batch_size=cfg['batch_size'],
                             pin_memory=True, num_workers=2, drop_last=True, sampler=trainsampler)
    
    valsampler = torch.utils.data.distributed.DistributedSampler(valset)
    valloader = DataLoader(valset, batch_size=1,
                             pin_memory=True, num_workers=2, drop_last=False, sampler=valsampler)
    total_iters = len(trainloader) * cfg['epochs']
    if "scheduler" not in cfg:
        logger.info("no scheduler used")
        scheduler_D = None
        scheduler_G = None
    elif cfg["scheduler"]["name"] == "PolynomialLR":
        logger.info("using PolynomialLR scheduler")
        scheduler_D = PolynomialLR(optimizer=optimizer_D, total_iters=total_iters, **cfg["scheduler"]["kwargs"])
        scheduler_G = PolynomialLR(optimizer=optimizer_G, total_iters=total_iters, **cfg["scheduler"]["kwargs"])
    elif cfg["scheduler"]["name"] == "WarmupCosineSchedule":
        logger.info("using WarmupCosineSchedule scheduler")
        scheduler_D = WarmupCosineSchedule(optimizer=optimizer_D, t_total=total_iters, **cfg["scheduler"]["kwargs"])
        scheduler_G = WarmupCosineSchedule(optimizer=optimizer_G, t_total=total_iters, **cfg["scheduler"]["kwargs"])
    elif cfg["scheduler"]["name"] == "CosineAnnealingLR":
        logger.info("using CosineAnnealingLR scheduler")
        cfg["scheduler"]["kwargs"]["T_max"] = total_iters
        scheduler_D = lr_scheduler.CosineAnnealingLR(optimizer=optimizer_D, **cfg["scheduler"]["kwargs"])
        scheduler_G = lr_scheduler.CosineAnnealingLR(optimizer=optimizer_G, **cfg["scheduler"]["kwargs"])
    else:
        logger.info(f"using {cfg['scheduler']['name']} scheduler")
        logger.info(cfg["scheduler"])
        scheduler_D = getattr(lr_scheduler, cfg["scheduler"]["name"])(optimizer=optimizer_D, **cfg["scheduler"]["kwargs"])
        scheduler_G = getattr(lr_scheduler, cfg["scheduler"]["name"])(optimizer=optimizer_G, **cfg["scheduler"]["kwargs"])
        
    local_rank = int(os.environ["LOCAL_RANK"])
    model_G_AB = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model_G_AB)
    model_G_BA = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model_G_BA)
    model_D_AB = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model_D_AB)
    model_D_BA = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model_D_BA)
    
    model_G_AB.cuda(local_rank)
    model_G_BA.cuda(local_rank)
    model_D_AB.cuda(local_rank)
    model_D_BA.cuda(local_rank)

    model_G_AB = torch.nn.parallel.DistributedDataParallel(model_G_AB, device_ids=[local_rank],
                                                      output_device=local_rank, find_unused_parameters=False)
    model_G_BA = torch.nn.parallel.DistributedDataParallel(model_G_BA, device_ids=[local_rank],
                                                      output_device=local_rank, find_unused_parameters=False)
    model_D_AB = torch.nn.parallel.DistributedDataParallel(model_D_AB, device_ids=[local_rank],
                                                      output_device=local_rank, find_unused_parameters=False)
    model_D_BA = torch.nn.parallel.DistributedDataParallel(model_D_BA, device_ids=[local_rank],
                                                      output_device=local_rank, find_unused_parameters=False)
    
    start_epoch = 0
    previous_best = 0.0
    
    criterionGAN = networks2d.GANLoss("lsgan")
    criterionCycle = torch.nn.L1Loss()
    criterionIdt = torch.nn.L1Loss()
    
    fake_A_pool = ImagePool(100)  # create image buffer to store previously generated images
    fake_B_pool = ImagePool(100)
            
    
    for epoch in range(start_epoch, cfg['epochs']):
        if rank == 0:
            logger.info('===========> Epoch: {:}, LR: {:.6f}, Previous best: {:.2f}'.format(
                epoch, optimizer_G.param_groups[0]['lr'], previous_best))

        total_loss_D = 0.0
        total_loss_G = 0.0

        trainsampler.set_epoch(epoch)
        
        for i, (real_A, real_B) in enumerate(trainloader):
            model_G_AB.train()
            model_G_BA.train()
            model_D_AB.train()
            model_D_BA.train()
            
            real_A, real_B = real_A.cuda(), real_B.cuda()
            #############################################
            #                   update G
            #############################################
            optimizer_G.zero_grad()
            fake_B = model_G_AB(real_A)  # G_A(A)
            rec_A = model_G_BA(fake_B)   # G_B(G_A(A))
            fake_A = model_G_BA(real_B)  # G_B(B)
            rec_B = model_G_AB(fake_A)   # G_A(G_B(B))
            
            lambda_idt = cfg["lambda_identity"]
            lambda_A = cfg["lambda_A"]
            lambda_B = cfg["lambda_B"]
            loss_idt_A = 0
            loss_idt_B = 0
            # Identity loss
            if lambda_idt > 0:
                # G_A should be identity if real_B is fed: ||G_A(B) - B||
                idt_A = model_G_AB(real_B)
                loss_idt_A = criterionIdt(idt_A, real_B) * lambda_B * lambda_idt
                # G_B should be identity if real_A is fed: ||G_B(A) - A||
                idt_B = model_G_BA(real_A)
                loss_idt_B = criterionIdt(idt_B, real_A) * lambda_A * lambda_idt


            # GAN loss D_A(G_A(A))
            loss_G_A = criterionGAN(model_D_AB(fake_B), True)
            # GAN loss D_B(G_B(B))
            loss_G_B = criterionGAN(model_D_BA(fake_A), True)
            # Forward cycle loss || G_B(G_A(A)) - A||
            loss_cycle_A = criterionCycle(rec_A, real_A) * lambda_A
            # Backward cycle loss || G_A(G_B(B)) - B||
            loss_cycle_B = criterionCycle(rec_B, real_B) * lambda_B
            # combined loss and calculate gradients
            loss_G = loss_G_A + loss_G_B + loss_cycle_A + loss_cycle_B + loss_idt_A + loss_idt_B
            loss_G.backward()

            if args.clip_grad_norm is not None:
                nn.utils.clip_grad_norm_(itertools.chain(model_G_AB.parameters(), model_G_BA.parameters()), args.clip_grad_norm)
            optimizer_G.step()
            total_loss_G += loss_G.item()
            
            
            #############################################
            #                   update D
            #############################################
            optimizer_D.zero_grad()

            fake_B = fake_B_pool.query(fake_B)
            loss_D_A = backward_D_basic(model_D_AB, real_B, fake_B, criterionGAN)
            fake_A = fake_A_pool.query(fake_A)
            loss_D_B = backward_D_basic(model_D_BA, real_A, fake_A, criterionGAN)
            optimizer_D.step()
            
            total_loss_D += loss_D_A.item() + loss_D_B.item()
            
            if "scheduler" in cfg and cfg["lr_decay_per_step"]:
                scheduler_D.step()
                scheduler_G.step()

            if ((i % 20) == 0) and (rank == 0):
                logger.info('Iters: {:}/ {:}, loss G: {:.3f}, loss D: {:.3f}'.format(i, len(trainloader), total_loss_G / (i+1), total_loss_D / (i+1)))


        if "scheduler" in cfg and cfg["lr_decay_per_epoch"]:
                scheduler_D.step()
                scheduler_G.step()
        
        model_G_AB.eval()
        with torch.no_grad():
            psnr, ssim, mse, l1 = \
                evaluate_2d(args, model_G_AB, valloader, True)
        
        if rank == 0:
            logger.info(
                '***** Evaluation ***** >>>> AVG (PSNR + SSIM):{:.2f} PSNR: {:.2f}, SSIM: {:.2f} MSE: {:.4f}, L1: {:.4f}\n'.format(sum([psnr, ssim]) / 2, psnr, ssim, mse, l1))

            if args.save_feq is not None and (epoch + 1) % args.save_feq == 0:
                torch.save({
                    "model_G_AB": model_G_AB.module.state_dict(),
                    "model_G_BA": model_G_BA.module.state_dict(),
                    "model_D_AB": model_D_AB.module.state_dict(),
                    "model_D_BA": model_D_BA.module.state_dict()},
                                os.path.join(args.save_path, f'epoch{epoch}.pth'))
            if sum([psnr, ssim]) / 2 > previous_best:
                if os.path.exists(os.path.join(args.save_path, 'best_{:.2f}.pth'.format(previous_best))):
                    os.remove(os.path.join(args.save_path, 'best_{:.2f}.pth'.format(previous_best)))
                previous_best = sum([psnr, ssim]) / 2
                torch.save({
                    "model_G_AB": model_G_AB.module.state_dict(),
                    "model_G_BA": model_G_BA.module.state_dict(),
                    "model_D_AB": model_D_AB.module.state_dict(),
                    "model_D_BA": model_D_BA.module.state_dict()},
                                os.path.join(args.save_path, 'best_{:.4f}.pth'.format(previous_best)))
    torch.save({
        "model_G_AB": model_G_AB.module.state_dict(),
        "model_G_BA": model_G_BA.module.state_dict(),
        "model_D_AB": model_D_AB.module.state_dict(),
        "model_D_BA": model_D_BA.module.state_dict()},
                    os.path.join(args.save_path, 'last.pth'))


if __name__ == '__main__':
    main()
